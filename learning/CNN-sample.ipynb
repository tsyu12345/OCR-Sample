{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNによるチェックマーク認識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像をグレースケールでロード\n",
    "image = cv2.imread('../sheet1.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方針\n",
    "ニューラルネットモデルを利用した、チェックボックスの検出とチェックマークの有無を判定する。\n",
    "が、方針として２通りがあると思われる。\n",
    "* ボックス検知を行うモデル、検出したボックスのチェックマークを判定するモデルの2つを作成する方法\n",
    "* ボックス検知とチェックマーク判定を同時に行う方法\n",
    "    * チェックマークを学習させ、画像中のチェックマークを検知させる方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画像中のチェックボックスを検出するモデルの作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#チェックボックスの検出モデルを作成\n",
    "# CNNアーキテクチャを定義\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  # 入力画像チャネル3、出力チャネル6、カーネルサイズ5\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)  # チェックマークと非チェックマークの2クラスを想定\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, json_file, transform=None):\n",
    "        self.data = []\n",
    "        with open(json_file, 'r') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = Image.open(requests.get(item['data_row']['row_data'], stream=True).raw)\n",
    "        objects = item['projects'][list(item['projects'].keys())[0]]['labels'][0]['annotations']['objects']\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in objects:\n",
    "            bbox = obj['bounding_box']\n",
    "            boxes.append([bbox['top'], bbox['left'], bbox['top'] + bbox['height'], bbox['left'] + bbox['width']])\n",
    "            labels.append(1 if obj['name'] == 'checked-box' else 0)  # Assuming 'checked-box' is label 1 and 'nonchecked-box' is label 0\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        if self.transform is not None:\n",
    "            image, target = self.transform(image, target)\n",
    "        return image, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_row': {'id': 'clkc190n608wj077n2xeq09sq', 'external_id': 'sheet1.jpg', 'row_data': 'https://storage.labelbox.com/clkc0y32f0hhh071s37v9c8xv%2F53b20677-2bb2-5323-dbc5-532ef383cb9d-sheet1.jpg?Expires=1689998324198&KeyName=labelbox-assets-key-3&Signature=HAtP_He0Xw_CARvC6cMmusNGHlk', 'details': {'dataset_id': 'clkc18z820elt07253e04gpkd', 'created_at': '2023-07-21T03:39:47.000+00:00', 'updated_at': '2023-07-21T03:39:47.000+00:00', 'last_activity_at': '2023-07-21T03:49:33.185+00:00', 'created_by': 'syukunt@gmail.com'}}, 'media_attributes': {'height': 1058, 'width': 748, 'mime_type': 'image/jpeg', 'exif_rotation': '1'}, 'attachments': [], 'metadata_fields': [], 'projects': {'clkc144x00eis07250u84cjom': {'name': 'CheckMarkDetection', 'labels': [{'label_kind': 'Default', 'version': '1.0.0', 'id': 'clkc1bszw0eob072502j80rn1', 'label_details': {'created_at': '2023-07-21T03:45:49.000+00:00', 'updated_at': '2023-07-21T03:45:49.000+00:00', 'created_by': 'syukunt@gmail.com', 'reviews': []}, 'performance_details': {'seconds_to_create': 224, 'seconds_to_review': 89, 'skipped': False}, 'annotations': {'objects': [{'feature_id': 'clkc1ccl500023b6lziznp7f4', 'feature_schema_id': 'clkc17c6r0hn5071sav6uc2r9', 'name': 'checked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 232.0, 'left': 432.0, 'height': 20.0, 'width': 20.0}}, {'feature_id': 'clkc1cr5000043b6lfkzcsyii', 'feature_schema_id': 'clkc17c6r0hn5071sav6uc2r9', 'name': 'checked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 232.0, 'left': 360.0, 'height': 20.0, 'width': 20.0}}, {'feature_id': 'clkc1cy7s00053b6lbcvjojou', 'feature_schema_id': 'clkc17c6r0hn5071sav6uc2r9', 'name': 'checked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 785.0, 'left': 184.0, 'height': 20.0, 'width': 20.0}}, {'feature_id': 'clkc1cye300063b6l2ngpptnv', 'feature_schema_id': 'clkc17c6r0hn5071sav6uc2r9', 'name': 'checked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 676.0, 'left': 186.0, 'height': 20.0, 'width': 20.0}}, {'feature_id': 'clkc1cyl100073b6lyjg2vv5t', 'feature_schema_id': 'clkc17c6r0hn5071sav6uc2r9', 'name': 'checked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 547.0, 'left': 186.0, 'height': 20.0, 'width': 20.0}}, {'feature_id': 'clkc1cyrp00083b6lat5f8l6y', 'feature_schema_id': 'clkc17c6r0hn5071sav6uc2r9', 'name': 'checked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 514.0, 'left': 185.0, 'height': 20.0, 'width': 20.0}}, {'feature_id': 'clkc1dxkb000a3b6lry3thesb', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 341.0, 'left': 51.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1e844000c3b6lkj86yvyp', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 276.0, 'left': 236.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1e8d0000d3b6l6nys34iy', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 276.0, 'left': 182.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1eg2c000e3b6lhf1113py', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 347.0, 'left': 324.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1egai000f3b6l6nckdqjo', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 347.0, 'left': 183.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1egk5000g3b6l3hgfg2sw', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 318.0, 'left': 429.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1egs5000h3b6lkq1dmlp5', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 319.0, 'left': 324.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1eh05000i3b6lgvy7bjej', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 319.0, 'left': 183.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1euvu000j3b6llamumpjd', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 390.0, 'left': 566.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1ezhk000k3b6l2cpf0w6k', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 391.0, 'left': 621.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1f6gk000l3b6l7czo3628', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 739.0, 'left': 185.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1f6mo000m3b6lr3y3pqg0', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 707.0, 'left': 184.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1f6sv000n3b6ld42728ir', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 642.0, 'left': 184.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1f6yc000o3b6l9iwnugr2', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 611.0, 'left': 185.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1f73e000p3b6lz7jp747p', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 578.0, 'left': 185.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1ftl0000q3b6ljw3abu46', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 963.0, 'left': 183.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1ftqf000r3b6lgkzdfjy0', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 784.0, 'left': 239.0, 'height': 22.0, 'width': 21.0}}, {'feature_id': 'clkc1g9yi000s3b6li0548xze', 'feature_schema_id': 'clkc17c6r0hn7071safhxhxzn', 'name': 'nonchecked-box', 'annotation_kind': 'ImageBoundingBox', 'classifications': [], 'bounding_box': {'top': 963.0, 'left': 240.0, 'height': 22.0, 'width': 21.0}}], 'classifications': [], 'relationships': []}}], 'project_details': {'ontology_id': 'clkc162yy0ghc070xa0zvhuhk', 'batch_id': '88ae30e0-2778-11ee-a8e2-e55fa656f375', 'priority': 1, 'consensus_expected_label_count': 1, 'workflow_history': [{'action': 'Move', 'created_at': '2023-07-21T03:49:33.192+00:00', 'created_by': 'syukunt@gmail.com', 'previous_task_name': 'Initial review task', 'previous_task_id': '02e60747-2484-4144-b04e-7ddd481e46c7'}, {'action': 'Move', 'created_at': '2023-07-21T03:45:48.662+00:00', 'created_by': 'syukunt@gmail.com', 'previous_task_name': 'Initial labeling task', 'previous_task_id': '6b5892a5-692c-0c7c-add1-6346f0453e04', 'next_task_name': 'Initial review task', 'next_task_id': '02e60747-2484-4144-b04e-7ddd481e46c7'}, {'action': 'Move', 'created_at': '2023-07-21T03:45:48.643+00:00', 'created_by': 'syukunt@gmail.com', 'next_task_name': 'Initial labeling task', 'next_task_id': '6b5892a5-692c-0c7c-add1-6346f0453e04'}]}}}}\n"
     ]
    }
   ],
   "source": [
    "with open('../export-result.ndjson', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(json.loads(line))\n",
    "        if i >= 2:  # Print the first 3 lines\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create the dataset and data loader\n",
    "dataset = CustomDataSet('../export-result.ndjson', transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Compose.__call__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):  \u001b[39m# loop over the dataset multiple times\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_loader, \u001b[39m0\u001b[39m):\n\u001b[1;32m     11\u001b[0m         \u001b[39mprint\u001b[39m(data)\n\u001b[1;32m     12\u001b[0m         \u001b[39m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n",
      "File \u001b[0;32m~/OCR-Sample/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/OCR-Sample/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/OCR-Sample/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/OCR-Sample/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[67], line 30\u001b[0m, in \u001b[0;36mCustomDataSet.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m target[\u001b[39m\"\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m image_id\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     image, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image, target)\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m image, target\n",
      "\u001b[0;31mTypeError\u001b[0m: Compose.__call__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# ネットワークを訓練\n",
    "# Initialize the network and optimizer\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        print(data)\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, _, labels = data  # We ignore the bounding boxes for now\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
